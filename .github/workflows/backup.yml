name: Daily Backup to Google Drive

on:
  schedule:
    - cron: '0 17 * * *'  # ë§¤ì¼ í•œêµ­ì‹œê°„ ìƒˆë²½ 2ì‹œ
  workflow_dispatch:  # ìˆ˜ë™ ì‹¤í–‰ ë²„íŠ¼

jobs:
  backup:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Install PostgreSQL Client
      run: |
        sudo apt-get update
        sudo apt-get install -y postgresql-client-14
    
    - name: Backup Database
      env:
        DATABASE_URL: ${{ secrets.SUPABASE_DB_URL }}
      run: |
        # ë°±ì—… ìƒì„±
        FILENAME="backup_$(date +%Y%m%d_%H%M%S).sql"
        pg_dump "$DATABASE_URL" --clean --no-owner --no-acl > $FILENAME
        
        # ì••ì¶•
        gzip $FILENAME
        echo "BACKUP_FILE=${FILENAME}.gz" >> $GITHUB_ENV
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        ls -lh ${FILENAME}.gz
    
    - name: Upload to Google Drive
      run: |
        # ì„œë¹„ìŠ¤ ê³„ì • íŒŒì¼ ìƒì„±
        echo '${{ secrets.GOOGLE_SERVICE_ACCOUNT }}' > service-account.json
        
        # Python ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ë° ì‹¤í–‰
        cat > upload.py << 'SCRIPT'
        import os
        from google.oauth2 import service_account
        from googleapiclient.discovery import build
        from googleapiclient.http import MediaFileUpload
        from datetime import datetime, timedelta
        
        # ì„¤ì •
        SCOPES = ['https://www.googleapis.com/auth/drive.file']
        SERVICE_ACCOUNT_FILE = 'service-account.json'
        FOLDER_ID = '${{ secrets.DRIVE_FOLDER_ID }}'
        BACKUP_FILE = os.environ['BACKUP_FILE']
        
        # ì¸ì¦
        credentials = service_account.Credentials.from_service_account_file(
            SERVICE_ACCOUNT_FILE, scopes=SCOPES)
        service = build('drive', 'v3', credentials=credentials)
        
        # íŒŒì¼ ì—…ë¡œë“œ
        file_metadata = {
            'name': BACKUP_FILE,
            'parents': [FOLDER_ID]
        }
        media = MediaFileUpload(BACKUP_FILE, mimetype='application/gzip')
        
        file = service.files().create(
            body=file_metadata,
            media_body=media,
            fields='id,name,size'
        ).execute()
        
        size_mb = int(file.get('size', 0)) / 1024 / 1024
        print(f"âœ… ë°±ì—… ì™„ë£Œ: {file.get('name')} ({size_mb:.2f} MB)")
        
        # 30ì¼ ì´ìƒ ëœ ë°±ì—… ì‚­ì œ
        cutoff = (datetime.now() - timedelta(days=30)).isoformat() + 'Z'
        
        response = service.files().list(
            q=f"'{FOLDER_ID}' in parents and createdTime < '{cutoff}' and name contains 'backup_'",
            fields='files(id, name)'
        ).execute()
        
        for old_file in response.get('files', []):
            service.files().delete(fileId=old_file['id']).execute()
            print(f"ðŸ—‘ï¸ ì‚­ì œ: {old_file['name']}")
        SCRIPT
        
        # Python íŒ¨í‚¤ì§€ ì„¤ì¹˜
        pip install --quiet google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client
        
        # ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        python upload.py
        
        # ì •ë¦¬
        rm -f service-account.json upload.py
    
    - name: Cleanup
      if: always()
      run: rm -f *.sql *.gz *.json *.py
